---
title: Organizers
nav: 2022
---

## Organizing Committee

<div class="id-pics" markdown="1">

- [Bahar Irfan](https://www.baharirfan.com/), *Evinoks Service Equipment Industry and Commerce Inc. (Turkey)*. 
{% include figure.html img="bahar-irfan.jpg" alt="Bahar Irfan" width="200px" height="200px" %}
Bahar Irfan is an R&D Associate at Evinoks Service Equipment Industry and Commerce Inc., working on the development of customizable software for industrial robots and smart buffets. She was an R&D Lab Associate at Disney Research Los Angeles (USA) in 2019, where she worked on dynamic emotional language adaptation in multiparty interactions. She received her PhD on multi-modal personalization in long-term human-robot interaction from the University of Plymouth (UK) and SoftBank Robotics Europe (France), as an Early-Stage Researcher in the Marie Skłodowska-Curie ITN project APRIL. She holds a BSc in mechanical engineering and an MSc in computer engineering from Bogazici University (Turkey). Her research focuses on creating personal robots that can continually learn and adapt to help people in everyday environments.

- [Aditi Ramachandran](http://www.aditiramachandran.com/), *Vän Robotics, USA*
{% include figure.html img="aditi-ramachandran.png" alt="Aditi Ramachandran" width="200px" height="200px" %}
Aditi Ramachandran is the Chief Robot Officer at Van Robotics where she works on building educational robots and oversees all software development at the company. She received a PhD from the Social Robotics Lab at Yale University where her research focused on personalized social robot tutors for children.

- [Samuel Spaulding](http://www.samspaulding.com/), *MIT Media Lab, USA*
{% include figure.html img="sam-spaulding.jpeg" alt="Samuel Spaulding" width="200px" height="200px" %}
Samuel Spaulding is a PhD student in the Personal Robots Group at the MIT Media Lab. His thesis research is focusing on building robots that can learn personalized cognitive and affective models of users over repeated interactions and across different tasks.


- [German I. Parisi](https://sites.google.com/view/giparisi/home), IBM Data and AI, Mountain View, CA
{% include figure.html img="parisi-21-g.png" alt="" width="200px" height="200px" %}
German I. Parisi is Program Director of Artificial Intelligence at IBM Data and AI in Mountain View, California. He is the co-founder of ContinualAI, the largest research organization on continual learning for AI with a network of over 1000 scientists. He received his PhD in Computer Science from the University of Hamburg, Germany in 2017. In 2015 he was a visiting researcher at the Cognitive Neuro-Robotics Lab of the Korea Advanced Institute of Science and Technology (KAIST), South Korea, winners of the 2015 DARPA Robotics Challenge. His review journal article on continual learning won the 2019 Best Paper Award by the International Neural Network Society. His main research interests include human-robot interaction, continual robot learning, and neuroscience-inspired AI.

- [Hatice Gunes](https://www.cl.cam.ac.uk/~hg410/), University of Cambridge, UK, Hatice.Gunes@cl.cam.ac.uk. 
{% include figure.html img="HG2019.jpg" alt="" width="200px" height="200px" %}
Dr. Gunes is a Reader in Affective Intelligence and Robotics (AFAR) and the Director of the AFAR Lab at the University of Cambridge. Her expertise is in the areas of affective computing and social signal processing cross-fertilizing research in multimodal interaction, computer vision, machine learning and social robotics. She has published over 100 papers in these areas (H-index=32, citations > 5,000) and her research highlights include Best Paper Candidate at IEEE RO-MAN’20, Outstanding Paper Award at IEEE FG’11 and Best Demo Award at IEEE ACII’09. Dr Gunes is the former President of the Association for the Advancement of Affective Computing, the General Co-Chair of ACII 2019, and the Program Co-Chair of ACM/IEEE HRI 2020 and IEEE FG 2017.


</div>

## Sponsors
<table>
    <tr>
    <td> <img src="images/epsrc.png" alt="EPSRC" style="width: 150px;"/> </td>    
    </tr>
</table>

